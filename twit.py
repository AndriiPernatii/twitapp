
FOLLOWERS_BY_USER = {109412083: [12312312, 12837291873]}
errored_users = []

TAGS_BY_USER = {"#dogs": 500}


def auth(name, password):
    """ Authorization """
    pass


def cache(func):
    def wrapper(*args, **kwargs):
        cache_key = 'cache-.json'

        if os.path.exists(cache_key):
            with open(cache_key) as fd:
                return json.load(fd)

        result = func(*args, **kwargs)
        with open(cache_key, 'w') as fd:
            json.dump(fd, result)
        return result

    return wrapper

#no threads in this function
# @cache
def get_target_subscribers(target_screen_name):
    """ Get a list of subscriber ids for the target account """

    api_path = 'https://api.twitter.com/1.1/followers/ids.json'
    payload = {'screen_name' : target_screen_name}

    request = requests.get(api_path , params=payload)
    followers = request.json()

    followers_ids = followers['ids']
    next_cursor = followers['next_cursor_str']

    #by default, GET followers/ids sends a request with cursor==-1, consider
    #implementing the whole function in the while-loop
    while next_cursor != 0:
        payload = {'screen_name' : target_screen_name, 'cursor' = next_cursor}
        request = requests.get(api_path , params=payload)
        # todo: check status code for request limits
        followers = request.json()
        followers_ids.extend(followers['ids'])
        next_cursor = followers['next_cursor_str']

    return followers_ids


def get_twit_tags(followers_ids):
    """ Get a list of tags from a liked post """
    likes_path = 'https://api.twitter.com/1.1/favorites/list.json'
    tags = []
    for id in followers_ids:
        payload = {'user_id' : id, 'count' : 200}
        request = requests.get(likes_path, params=payload)
        likes = request.json()
        # todo: what about pagination?
        for like in likes:
            twit_tags = like['entities']['hashtags']
            tags.extend(twit_tags)

    # collections.Counter(tags)
    return tags


def update_result_dict(result_dict, subresult_dict):
    """ Update the result_dict with values from each subresult generated by
    a worker """

    for key, value in subresult_dict.items():
        result_dict[key] += value
    return result_dict


def genarate_random_dict():
    import random, logging
    result = {}
    test_tags = ['cats', 'dogs', 'birds', 'obama', ]
    for tag in random.sample(test_tags, 2):
        result[tag] = random.randint(1, 10)
    logging.info("Generated", result)
    return result


def worker(id_list, result_dict, result_lock):
    """ Get a dict with tag counts for each post"""
    subresult_dict = generate_random_dict()

    #updating the result_dict
    with result_lock:
        update_result_dict(result_dict, subresult_dict)

    return result_dict


def main():
    import Queue
    import threading
    import json
    import time
    import requests
    from collections import defaultdict

    result_dict = defaultdict(int)
    result_lock = threading.Lock()
    #thread-safe queues
    work_queue = Queue.Queue()
    results_queue = Queue.Queue()

    threads = []

    logging.info("Creating threads")
    for _ in range(20):
        thread = threading.Thread(target=worker, args=([], result_dict, result_lock))
        logging.info("Starting threads")
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()

    with open(f"tags_{time.time()}.json", "w") as file:
        json.dump(result_dict, file, indent = 2)

    return result_dict


if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    print(main())
