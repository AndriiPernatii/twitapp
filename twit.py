
FOLLOWERS_BY_USER = {109412083: [12312312, 12837291873]}
errored_users = []

TAGS_BY_USER = {"#dogs": 500}

FOLLOWERS_CONNECTIONS = 15 #per 15 minutes
TWIT_TAGS_CONNECTIONS = 75 #per 15 minutes
#API endpoints to be used in GET requests
API_PATH = 'https://api.twitter.com/1.1/followers/ids.json'
LIKES_PATH = 'https://api.twitter.com/1.1/favorites/list.json'

def auth(name, password):
    """ Authorization """
    pass


def cache(func):
    def wrapper(*args, **kwargs):
        cache_key = 'cache-.json'

        if os.path.exists(cache_key):
            with open(cache_key) as fd:
                return json.load(fd)

        result = func(*args, **kwargs)
        with open(cache_key, 'w') as fd:
            json.dump(fd, result)
        return result

    return wrapper

#no threads in this function
# @cache
def get_target_subscribers(target_screen_name):
    """ Get a list of subscriber ids for the target account """

    payload = {'screen_name' : target_screen_name}
    request = requests.get(API_PATH , params=payload)
    followers = request.json()

    followers_ids = followers['ids']
    next_cursor = followers['next_cursor_str']

    #by default, GET followers/ids sends a request with cursor==-1, consider
    #implementing the whole function in the while-loop
    while next_cursor != 0:
        payload = {'screen_name' : target_screen_name, 'cursor' = next_cursor}
        request = requests.get(API_PATH , params=payload)
        # todo: check status code for request limits
        followers = request.json()
        followers_ids.extend(followers['ids'])
        next_cursor = followers['next_cursor_str']

    return followers_ids

def get_next(queue):
    try:
        id = work_queue.get()
        work_queue.task_done()
    except:
        logging.info("Could not retrieve an id from the working queue")
    return id


def get_twit_tags(twit_tags_semaphore, work_queue, results_queue):
    """ Get a list of tags from a liked post """
    tags = []
    id = get_next(work_queue)
    with twit_tags_semaphore:
        try:
            payload = {'user_id' : id, 'count' : 200}
            request = requests.get(LIKES_PATH, params=payload)
            likes = request.json()
            # todo: what about pagination?
            for like in likes:
                twit_tags = like['entities']['hashtags']
                tags.extend(twit_tags)
        except: #add rate limit exception returned by the API and recursive call to itself if exception
            logging.info("Could not retrieve data from Twitter due to rate limit")
        new_result_entry = Counter(tags)
        results_queue.put(new_result_entry)
        return


def update_result_dict(result_dict, subresult_dict):
    """ Update the result_dict with values from each subresult generated by
    a worker """

    for key, value in subresult_dict.items():
        result_dict[key] += value
    return result_dict


def genarate_random_dict():
    import random, logging
    result = {}
    test_tags = ['cats', 'dogs', 'birds', 'obama', ]
    for tag in random.sample(test_tags, 2):
        result[tag] = random.randint(1, 10)
    logging.info("Generated", result)
    return result


def worker(id_list, result_dict, result_lock):
    """ Get a dict with tag counts for each post"""
    subresult_dict = generate_random_dict()

    #updating the result_dict
    with result_lock:
        update_result_dict(result_dict, subresult_dict)

    return result_dict


def main():
    import Queue
    import threading
    import json
    import time
    import requests
    from collections import defaultdict, Counter

    result_dict = defaultdict(int)
    result_lock = threading.Lock()
    #thread-safe queues
    work_queue = Queue.Queue()
    results_queue = Queue.Queue()

    twit_tags_semaphore = threading.BoundedSemaphore(value = TWIT_TAGS_CONNECTIONS)
    #add followers' ids to the work_queue for further processing by worker threads
    followers_ids = get_target_subscribers("")
    for id in followers_ids:
        work_queue.put(id)

    threads = []
    tag_threads = []

    logging.info("Creating threads to get tags")
    for _ in range(len(followers_ids)):
        thread = threading.Thread(target=get_twit_tags, args=(twit_tags_semaphore, work_queue))
        logging.info("Starting threads to get tags")
        thread.start()
        tag_threads.append(thread)

    for thread in tag_threads:
        thread.join()

    logging.info("Creating threads to merge counters")
    for _ in range(20):
        thread = threading.Thread(target=worker, args=([], result_dict, result_lock))
        logging.info("Starting threads")
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()

    with open(f"tags_{time.time()}.json", "w") as file:
        json.dump(result_dict, file, indent = 2)

    return result_dict


if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    print(main())
